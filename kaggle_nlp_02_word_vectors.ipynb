{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Word Embeddings\n",
    "\n",
    "- machine learning on text requires that you first represent the text numerically\n",
    "\n",
    "    - bag of words representation is one way\n",
    "    \n",
    "    - can usually do better with word embeddings\n",
    "\n",
    "- **Word embeddings** (also called **word vectors**) \n",
    "\n",
    "    - represent each word numerically in a way \n",
    "\n",
    "        - that the vector corresponds to how that word is used or what it means\n",
    "\n",
    "    - vector encodings are learned by considering the context in which the words appear\n",
    "\n",
    "    - words that appear in similar contexts will have similar vectors\n",
    "    \n",
    "        - For example, vectors for \"leopard\", \"lion\", and \"tiger\" will be close together, \n",
    "            \n",
    "            - while they'll be far away from \"planet\" and \"castle\"\n",
    "\n",
    "- These vectors can be used as features for machine learning models\n",
    "\n",
    "    - Word vectors will typically improve the performance of your models above bag of words encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### SpaCy Word Embeddings\n",
    "\n",
    "- spaCy provides embeddings learned from a model called `Word2Vec`\n",
    "\n",
    "- access them by loading a large language model like `en_core_web_lg`\n",
    "    \n",
    "    - they will be available on tokens from the `.vector` attribute"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy \n",
    "import numpy as np\n",
    "\n",
    "# import spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a large model to get word vectors \n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(12, 300)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# define the text to analyze\n",
    "text_to_analyze = \"These vectors can be used as features for machine learning models.\"\n",
    "\n",
    "# for the rest of this exercise, always work with disabled pipes to speed up the process a bit and also because we dont need the pipes \n",
    "with nlp.disable_pipes():\n",
    "\n",
    "    # get the word vectors for the words in the sectence of the text to analyze\n",
    "    vectors = np.array( [ token.vector for token in nlp(text_to_analyze) ] )\n",
    "\n",
    "# check the shape of the newly created array\n",
    "vectors.shape"
   ]
  },
  {
   "source": [
    "- These are 300-dimensional vectors, with one vector for each word\n",
    "\n",
    "- However, we only have document-level labels and our models won't be able to use the word-level embeddings\n",
    "\n",
    "    - So, you need a vector representation for the entire document"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Document Level Vectors\n",
    "\n",
    "- there are many ways to combine all the word vectors into a single document vector we can use for model training\n",
    "\n",
    "- a simple and surprisingly effective approach is simply averaging the vectors for each word in the document\n",
    "\n",
    "    - then, you can use these document vectors for modeling\n",
    "\n",
    "- spaCy calculates the average document vector which you can get with `doc.vector`\n",
    "    \n",
    "    - Here is an example loading the spam data and converting it to document vectors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5572, 300)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# load the spam/ham data \n",
    "spam = pd.read_csv('kaggle_data/spam.csv')\n",
    "\n",
    "# disable pipes again\n",
    "with nlp.disable_pipes():\n",
    "\n",
    "    # generate the document vectors \n",
    "    doc_vectors = np.array( [ nlp(text).vector for text in spam.text ] )\n",
    "\n",
    "doc_vectors.shape"
   ]
  },
  {
   "source": [
    "# Classification Models\n",
    "\n",
    "- with document vectors, scikit-learn models, xgboost model and other standard models can be trained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_vectors, spam.label,\n",
    "                                                    test_size=0.1, random_state=1)"
   ]
  },
  {
   "source": [
    "- Scikit-learn provides an SVM classifier LinearSVC. This works similar to other scikit-learn models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 97.312%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Set dual=False to speed up training, and it's not needed\n",
    "svc = LinearSVC(random_state=1, dual=False, max_iter=10000)\n",
    "svc.fit(X_train, y_train)\n",
    "print(f\"Accuracy: {svc.score(X_test, y_test) * 100:.3f}%\", )"
   ]
  },
  {
   "source": [
    "### Document Similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- Documents with similar content generally have similar vectors\n",
    "\n",
    "- So you can find similar documents by measuring the similarity between the vectors\n",
    "\n",
    "- A common metric for this is the cosine similarity which measures the angle between two vectors,  $ ùêö $  and  $ ùêõ $ \n",
    "\n",
    "$$\n",
    "\n",
    "\\cos{\\theta} = \\frac{a \\cdot b}{\\lvert\\lvert a \\rvert\\rvert \\lvert\\lvert b \\rvert\\rvert}\n",
    "\n",
    "$$\n",
    "\n",
    "- The cosine similarity can vary between -1 and 1, corresponding complete opposite to perfect similarity, respectively"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# define function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(a, b):\n",
    "    return a.dot(b)/np.sqrt(a.dot(a) * b.dot(b))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7030031"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# initilize two docuemnt vectors \n",
    "a = nlp(\"REPLY NOW FOR FREE TEA\").vector\n",
    "b = nlp(\"According to legend, Emperor Shen Nung discovered tea when leaves from a wild tree blew into his pot of boiling water.\").vector\n",
    "\n",
    "# compute cosine similarity between the two vectors \n",
    "cosine_similarity(a, b)"
   ]
  }
 ]
}